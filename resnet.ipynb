{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Neural Network - ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "torch.random.seed = 39\n",
    "np.random.seed = 39\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)), transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "test_transform = transforms. Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4, Classes: ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2870,\n",
       " 394,\n",
       " ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DIR = 'archive/Training'\n",
    "TEST_DIR = 'archive/Testing'\n",
    "\n",
    "# Load the training dataset\n",
    "full_train_dataset = torchvision.datasets.ImageFolder (root=TRAIN_DIR, transform=train_transform)\n",
    "# Load the testing dataset\n",
    "test_dataset = torchvision.datasets. ImageFolder(root=TEST_DIR, transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "classes = full_train_dataset.classes\n",
    "print(f\"Number of classes: {len(classes)}, Classes: {classes}\")\n",
    "len (full_train_dataset), len(test_dataset), classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sizes for training and validation splits\n",
    "train_size = int(0.9 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Use DataLoader to load the splits\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels))\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ReLU())\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.max_pool(out)\n",
    "        out = self.layer0(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = ResNet(Block, [3, 4, 6, 3]).to(device)\n",
    "model = torchvision.models.resnet34()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_epochs = 11\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "#Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/11], Step [1/81], Loss: 0.8382\n",
      "Epoch [1/11], Step [2/81], Loss: 0.7440\n",
      "Epoch [1/11], Step [3/81], Loss: 1.4683\n",
      "Epoch [1/11], Step [4/81], Loss: 1.4109\n",
      "Epoch [1/11], Step [5/81], Loss: 1.1802\n",
      "Epoch [1/11], Step [6/81], Loss: 0.8975\n",
      "Epoch [1/11], Step [7/81], Loss: 1.0811\n",
      "Epoch [1/11], Step [8/81], Loss: 1.1744\n",
      "Epoch [1/11], Step [9/81], Loss: 1.0649\n",
      "Epoch [1/11], Step [10/81], Loss: 1.0662\n",
      "Epoch [1/11], Step [11/81], Loss: 1.1940\n",
      "Epoch [1/11], Step [12/81], Loss: 1.2311\n",
      "Epoch [1/11], Step [13/81], Loss: 0.8771\n",
      "Epoch [1/11], Step [14/81], Loss: 0.9905\n",
      "Epoch [1/11], Step [15/81], Loss: 1.2179\n",
      "Epoch [1/11], Step [16/81], Loss: 0.9431\n",
      "Epoch [1/11], Step [17/81], Loss: 1.1154\n",
      "Epoch [1/11], Step [18/81], Loss: 1.0262\n",
      "Epoch [1/11], Step [19/81], Loss: 1.1126\n",
      "Epoch [1/11], Step [20/81], Loss: 0.8413\n",
      "Epoch [1/11], Step [21/81], Loss: 0.9634\n",
      "Epoch [1/11], Step [22/81], Loss: 0.8857\n",
      "Epoch [1/11], Step [23/81], Loss: 0.9613\n",
      "Epoch [1/11], Step [24/81], Loss: 0.7380\n",
      "Epoch [1/11], Step [25/81], Loss: 0.8087\n",
      "Epoch [1/11], Step [26/81], Loss: 1.0084\n",
      "Epoch [1/11], Step [27/81], Loss: 1.1128\n",
      "Epoch [1/11], Step [28/81], Loss: 1.0260\n",
      "Epoch [1/11], Step [29/81], Loss: 0.7944\n",
      "Epoch [1/11], Step [30/81], Loss: 0.8556\n",
      "Epoch [1/11], Step [31/81], Loss: 1.0338\n",
      "Epoch [1/11], Step [32/81], Loss: 0.8715\n",
      "Epoch [1/11], Step [33/81], Loss: 0.9143\n",
      "Epoch [1/11], Step [34/81], Loss: 0.8275\n",
      "Epoch [1/11], Step [35/81], Loss: 0.8266\n",
      "Epoch [1/11], Step [36/81], Loss: 1.0264\n",
      "Epoch [1/11], Step [37/81], Loss: 0.9168\n",
      "Epoch [1/11], Step [38/81], Loss: 0.9703\n",
      "Epoch [1/11], Step [39/81], Loss: 0.8195\n",
      "Epoch [1/11], Step [40/81], Loss: 0.9563\n",
      "Epoch [1/11], Step [41/81], Loss: 0.7561\n",
      "Epoch [1/11], Step [42/81], Loss: 0.6947\n",
      "Epoch [1/11], Step [43/81], Loss: 0.8291\n",
      "Epoch [1/11], Step [44/81], Loss: 0.7690\n",
      "Epoch [1/11], Step [45/81], Loss: 1.0406\n",
      "Epoch [1/11], Step [46/81], Loss: 0.6579\n",
      "Epoch [1/11], Step [47/81], Loss: 0.8804\n",
      "Epoch [1/11], Step [48/81], Loss: 0.9627\n",
      "Epoch [1/11], Step [49/81], Loss: 0.7594\n",
      "Epoch [1/11], Step [50/81], Loss: 1.0665\n",
      "Epoch [1/11], Step [51/81], Loss: 0.6956\n",
      "Epoch [1/11], Step [52/81], Loss: 0.7103\n",
      "Epoch [1/11], Step [53/81], Loss: 0.7652\n",
      "Epoch [1/11], Step [54/81], Loss: 0.8036\n",
      "Epoch [1/11], Step [55/81], Loss: 0.8355\n",
      "Epoch [1/11], Step [56/81], Loss: 0.8724\n",
      "Epoch [1/11], Step [57/81], Loss: 0.8277\n",
      "Epoch [1/11], Step [58/81], Loss: 0.4747\n",
      "Epoch [1/11], Step [59/81], Loss: 0.8768\n",
      "Epoch [1/11], Step [60/81], Loss: 0.6974\n",
      "Epoch [1/11], Step [61/81], Loss: 0.6569\n",
      "Epoch [1/11], Step [62/81], Loss: 1.2533\n",
      "Epoch [1/11], Step [63/81], Loss: 0.6590\n",
      "Epoch [1/11], Step [64/81], Loss: 0.7124\n",
      "Epoch [1/11], Step [65/81], Loss: 0.6863\n",
      "Epoch [1/11], Step [66/81], Loss: 0.9370\n",
      "Epoch [1/11], Step [67/81], Loss: 0.7826\n",
      "Epoch [1/11], Step [68/81], Loss: 0.9639\n",
      "Epoch [1/11], Step [69/81], Loss: 0.9762\n",
      "Epoch [1/11], Step [70/81], Loss: 0.7597\n",
      "Epoch [1/11], Step [71/81], Loss: 0.8313\n",
      "Epoch [1/11], Step [72/81], Loss: 0.8996\n",
      "Epoch [1/11], Step [73/81], Loss: 0.7918\n",
      "Epoch [1/11], Step [74/81], Loss: 1.0409\n",
      "Epoch [1/11], Step [75/81], Loss: 0.8938\n",
      "Epoch [1/11], Step [76/81], Loss: 0.8334\n",
      "Epoch [1/11], Step [77/81], Loss: 0.6164\n",
      "Epoch [1/11], Step [78/81], Loss: 0.7788\n",
      "Epoch [1/11], Step [79/81], Loss: 0.6501\n",
      "Epoch [1/11], Step [80/81], Loss: 0.8931\n",
      "Epoch [1/11], Step [81/81], Loss: 0.8952\n",
      "Epoch [2/11], Step [1/81], Loss: 0.7275\n",
      "Epoch [2/11], Step [2/81], Loss: 1.0796\n",
      "Epoch [2/11], Step [3/81], Loss: 0.6877\n",
      "Epoch [2/11], Step [4/81], Loss: 0.9550\n",
      "Epoch [2/11], Step [5/81], Loss: 0.7768\n",
      "Epoch [2/11], Step [6/81], Loss: 0.7648\n",
      "Epoch [2/11], Step [7/81], Loss: 0.7529\n",
      "Epoch [2/11], Step [8/81], Loss: 0.6386\n",
      "Epoch [2/11], Step [9/81], Loss: 1.0024\n",
      "Epoch [2/11], Step [10/81], Loss: 0.5728\n",
      "Epoch [2/11], Step [11/81], Loss: 0.9385\n",
      "Epoch [2/11], Step [12/81], Loss: 0.6992\n",
      "Epoch [2/11], Step [13/81], Loss: 1.0739\n",
      "Epoch [2/11], Step [14/81], Loss: 0.8454\n",
      "Epoch [2/11], Step [15/81], Loss: 0.7843\n",
      "Epoch [2/11], Step [16/81], Loss: 0.6602\n",
      "Epoch [2/11], Step [17/81], Loss: 0.7204\n",
      "Epoch [2/11], Step [18/81], Loss: 0.7433\n",
      "Epoch [2/11], Step [19/81], Loss: 0.7406\n",
      "Epoch [2/11], Step [20/81], Loss: 0.8382\n",
      "Epoch [2/11], Step [21/81], Loss: 0.9331\n",
      "Epoch [2/11], Step [22/81], Loss: 0.7935\n",
      "Epoch [2/11], Step [23/81], Loss: 0.8402\n",
      "Epoch [2/11], Step [24/81], Loss: 0.8198\n",
      "Epoch [2/11], Step [25/81], Loss: 0.5571\n",
      "Epoch [2/11], Step [26/81], Loss: 0.7178\n",
      "Epoch [2/11], Step [27/81], Loss: 0.7209\n",
      "Epoch [2/11], Step [28/81], Loss: 0.7051\n",
      "Epoch [2/11], Step [29/81], Loss: 0.6410\n",
      "Epoch [2/11], Step [30/81], Loss: 1.0253\n",
      "Epoch [2/11], Step [31/81], Loss: 0.7454\n",
      "Epoch [2/11], Step [32/81], Loss: 0.8595\n",
      "Epoch [2/11], Step [33/81], Loss: 0.5322\n",
      "Epoch [2/11], Step [34/81], Loss: 0.8937\n",
      "Epoch [2/11], Step [35/81], Loss: 0.9229\n",
      "Epoch [2/11], Step [36/81], Loss: 0.7037\n",
      "Epoch [2/11], Step [37/81], Loss: 1.1497\n",
      "Epoch [2/11], Step [38/81], Loss: 0.6471\n",
      "Epoch [2/11], Step [39/81], Loss: 0.7036\n",
      "Epoch [2/11], Step [40/81], Loss: 0.6577\n",
      "Epoch [2/11], Step [41/81], Loss: 0.9018\n",
      "Epoch [2/11], Step [42/81], Loss: 0.7053\n",
      "Epoch [2/11], Step [43/81], Loss: 0.6918\n",
      "Epoch [2/11], Step [44/81], Loss: 0.6634\n",
      "Epoch [2/11], Step [45/81], Loss: 0.6914\n",
      "Epoch [2/11], Step [46/81], Loss: 0.5150\n",
      "Epoch [2/11], Step [47/81], Loss: 0.7547\n",
      "Epoch [2/11], Step [48/81], Loss: 0.8526\n",
      "Epoch [2/11], Step [49/81], Loss: 0.7043\n",
      "Epoch [2/11], Step [50/81], Loss: 1.0548\n",
      "Epoch [2/11], Step [51/81], Loss: 0.6518\n",
      "Epoch [2/11], Step [52/81], Loss: 0.6083\n",
      "Epoch [2/11], Step [53/81], Loss: 0.6863\n",
      "Epoch [2/11], Step [54/81], Loss: 0.8518\n",
      "Epoch [2/11], Step [55/81], Loss: 0.7563\n",
      "Epoch [2/11], Step [56/81], Loss: 0.7164\n",
      "Epoch [2/11], Step [57/81], Loss: 0.5996\n",
      "Epoch [2/11], Step [58/81], Loss: 0.7570\n",
      "Epoch [2/11], Step [59/81], Loss: 0.7319\n",
      "Epoch [2/11], Step [60/81], Loss: 0.8151\n",
      "Epoch [2/11], Step [61/81], Loss: 0.7300\n",
      "Epoch [2/11], Step [62/81], Loss: 0.5343\n",
      "Epoch [2/11], Step [63/81], Loss: 0.7900\n",
      "Epoch [2/11], Step [64/81], Loss: 0.6091\n",
      "Epoch [2/11], Step [65/81], Loss: 0.6821\n",
      "Epoch [2/11], Step [66/81], Loss: 0.7180\n",
      "Epoch [2/11], Step [67/81], Loss: 0.6688\n",
      "Epoch [2/11], Step [68/81], Loss: 1.0634\n",
      "Epoch [2/11], Step [69/81], Loss: 0.7565\n",
      "Epoch [2/11], Step [70/81], Loss: 0.7872\n",
      "Epoch [2/11], Step [71/81], Loss: 0.8192\n",
      "Epoch [2/11], Step [72/81], Loss: 0.5171\n",
      "Epoch [2/11], Step [73/81], Loss: 0.7740\n",
      "Epoch [2/11], Step [74/81], Loss: 0.8234\n",
      "Epoch [2/11], Step [75/81], Loss: 0.7178\n",
      "Epoch [2/11], Step [76/81], Loss: 0.5094\n",
      "Epoch [2/11], Step [77/81], Loss: 0.8111\n",
      "Epoch [2/11], Step [78/81], Loss: 1.2015\n",
      "Epoch [2/11], Step [79/81], Loss: 0.6647\n",
      "Epoch [2/11], Step [80/81], Loss: 0.9666\n",
      "Epoch [2/11], Step [81/81], Loss: 0.7035\n",
      "Epoch [3/11], Step [1/81], Loss: 0.7466\n",
      "Epoch [3/11], Step [2/81], Loss: 0.5361\n",
      "Epoch [3/11], Step [3/81], Loss: 0.6690\n",
      "Epoch [3/11], Step [4/81], Loss: 0.8536\n",
      "Epoch [3/11], Step [5/81], Loss: 0.8707\n",
      "Epoch [3/11], Step [6/81], Loss: 0.6199\n",
      "Epoch [3/11], Step [7/81], Loss: 0.6771\n",
      "Epoch [3/11], Step [8/81], Loss: 0.7259\n",
      "Epoch [3/11], Step [9/81], Loss: 0.8043\n",
      "Epoch [3/11], Step [10/81], Loss: 0.7946\n",
      "Epoch [3/11], Step [11/81], Loss: 0.7559\n",
      "Epoch [3/11], Step [12/81], Loss: 0.7546\n",
      "Epoch [3/11], Step [13/81], Loss: 0.7987\n",
      "Epoch [3/11], Step [14/81], Loss: 0.6319\n",
      "Epoch [3/11], Step [15/81], Loss: 0.6977\n",
      "Epoch [3/11], Step [16/81], Loss: 0.7027\n",
      "Epoch [3/11], Step [17/81], Loss: 0.6625\n",
      "Epoch [3/11], Step [18/81], Loss: 0.8194\n",
      "Epoch [3/11], Step [19/81], Loss: 0.7512\n",
      "Epoch [3/11], Step [20/81], Loss: 0.6744\n",
      "Epoch [3/11], Step [21/81], Loss: 0.8611\n",
      "Epoch [3/11], Step [22/81], Loss: 0.4399\n",
      "Epoch [3/11], Step [23/81], Loss: 0.6663\n",
      "Epoch [3/11], Step [24/81], Loss: 0.8395\n",
      "Epoch [3/11], Step [25/81], Loss: 0.5754\n",
      "Epoch [3/11], Step [26/81], Loss: 0.7161\n",
      "Epoch [3/11], Step [27/81], Loss: 0.8702\n",
      "Epoch [3/11], Step [28/81], Loss: 0.8262\n",
      "Epoch [3/11], Step [29/81], Loss: 0.5990\n",
      "Epoch [3/11], Step [30/81], Loss: 0.7993\n",
      "Epoch [3/11], Step [31/81], Loss: 0.5814\n",
      "Epoch [3/11], Step [32/81], Loss: 0.7775\n",
      "Epoch [3/11], Step [33/81], Loss: 0.5721\n",
      "Epoch [3/11], Step [34/81], Loss: 0.7633\n",
      "Epoch [3/11], Step [35/81], Loss: 0.5459\n",
      "Epoch [3/11], Step [36/81], Loss: 0.6929\n",
      "Epoch [3/11], Step [37/81], Loss: 0.4949\n",
      "Epoch [3/11], Step [38/81], Loss: 0.7263\n",
      "Epoch [3/11], Step [39/81], Loss: 0.4522\n",
      "Epoch [3/11], Step [40/81], Loss: 0.8712\n",
      "Epoch [3/11], Step [41/81], Loss: 0.8847\n",
      "Epoch [3/11], Step [42/81], Loss: 0.8346\n",
      "Epoch [3/11], Step [43/81], Loss: 0.5249\n",
      "Epoch [3/11], Step [44/81], Loss: 0.7702\n",
      "Epoch [3/11], Step [45/81], Loss: 0.7089\n",
      "Epoch [3/11], Step [46/81], Loss: 0.5244\n",
      "Epoch [3/11], Step [47/81], Loss: 0.8405\n",
      "Epoch [3/11], Step [48/81], Loss: 0.5362\n",
      "Epoch [3/11], Step [49/81], Loss: 0.6655\n",
      "Epoch [3/11], Step [50/81], Loss: 0.7588\n",
      "Epoch [3/11], Step [51/81], Loss: 0.5952\n",
      "Epoch [3/11], Step [52/81], Loss: 0.6826\n",
      "Epoch [3/11], Step [53/81], Loss: 0.7380\n",
      "Epoch [3/11], Step [54/81], Loss: 0.5152\n",
      "Epoch [3/11], Step [55/81], Loss: 0.8615\n",
      "Epoch [3/11], Step [56/81], Loss: 0.5187\n",
      "Epoch [3/11], Step [57/81], Loss: 0.5995\n",
      "Epoch [3/11], Step [58/81], Loss: 0.6900\n",
      "Epoch [3/11], Step [59/81], Loss: 0.6722\n",
      "Epoch [3/11], Step [60/81], Loss: 0.6045\n",
      "Epoch [3/11], Step [61/81], Loss: 0.7365\n",
      "Epoch [3/11], Step [62/81], Loss: 0.7978\n",
      "Epoch [3/11], Step [63/81], Loss: 0.5471\n",
      "Epoch [3/11], Step [64/81], Loss: 0.7317\n",
      "Epoch [3/11], Step [65/81], Loss: 0.7823\n",
      "Epoch [3/11], Step [66/81], Loss: 0.5279\n",
      "Epoch [3/11], Step [67/81], Loss: 1.4247\n",
      "Epoch [3/11], Step [68/81], Loss: 0.6418\n",
      "Epoch [3/11], Step [69/81], Loss: 0.8071\n",
      "Epoch [3/11], Step [70/81], Loss: 0.9764\n",
      "Epoch [3/11], Step [71/81], Loss: 0.5714\n",
      "Epoch [3/11], Step [72/81], Loss: 0.5988\n",
      "Epoch [3/11], Step [73/81], Loss: 0.6733\n",
      "Epoch [3/11], Step [74/81], Loss: 0.6102\n",
      "Epoch [3/11], Step [75/81], Loss: 0.6123\n",
      "Epoch [3/11], Step [76/81], Loss: 0.8304\n",
      "Epoch [3/11], Step [77/81], Loss: 0.7346\n",
      "Epoch [3/11], Step [78/81], Loss: 0.5856\n",
      "Epoch [3/11], Step [79/81], Loss: 0.6366\n",
      "Epoch [3/11], Step [80/81], Loss: 0.5377\n",
      "Epoch [3/11], Step [81/81], Loss: 0.6903\n",
      "Epoch [4/11], Step [1/81], Loss: 0.6293\n",
      "Epoch [4/11], Step [2/81], Loss: 0.8081\n",
      "Epoch [4/11], Step [3/81], Loss: 0.6160\n",
      "Epoch [4/11], Step [4/81], Loss: 0.7636\n",
      "Epoch [4/11], Step [5/81], Loss: 0.4235\n",
      "Epoch [4/11], Step [6/81], Loss: 0.7600\n",
      "Epoch [4/11], Step [7/81], Loss: 0.5692\n",
      "Epoch [4/11], Step [8/81], Loss: 0.8317\n",
      "Epoch [4/11], Step [9/81], Loss: 0.5654\n",
      "Epoch [4/11], Step [10/81], Loss: 0.4323\n",
      "Epoch [4/11], Step [11/81], Loss: 0.6746\n",
      "Epoch [4/11], Step [12/81], Loss: 0.5114\n",
      "Epoch [4/11], Step [13/81], Loss: 0.7156\n",
      "Epoch [4/11], Step [14/81], Loss: 0.4836\n",
      "Epoch [4/11], Step [15/81], Loss: 0.5360\n",
      "Epoch [4/11], Step [16/81], Loss: 0.8414\n",
      "Epoch [4/11], Step [17/81], Loss: 0.5667\n",
      "Epoch [4/11], Step [18/81], Loss: 0.7611\n",
      "Epoch [4/11], Step [19/81], Loss: 0.5234\n",
      "Epoch [4/11], Step [20/81], Loss: 0.5629\n",
      "Epoch [4/11], Step [21/81], Loss: 0.8246\n",
      "Epoch [4/11], Step [22/81], Loss: 0.5404\n",
      "Epoch [4/11], Step [23/81], Loss: 0.4531\n",
      "Epoch [4/11], Step [24/81], Loss: 0.6707\n",
      "Epoch [4/11], Step [25/81], Loss: 0.3648\n",
      "Epoch [4/11], Step [26/81], Loss: 0.5849\n",
      "Epoch [4/11], Step [27/81], Loss: 0.5678\n",
      "Epoch [4/11], Step [28/81], Loss: 0.7629\n",
      "Epoch [4/11], Step [29/81], Loss: 0.6856\n",
      "Epoch [4/11], Step [30/81], Loss: 0.6163\n",
      "Epoch [4/11], Step [31/81], Loss: 0.4363\n",
      "Epoch [4/11], Step [32/81], Loss: 0.7009\n",
      "Epoch [4/11], Step [33/81], Loss: 0.6388\n",
      "Epoch [4/11], Step [34/81], Loss: 0.5863\n",
      "Epoch [4/11], Step [35/81], Loss: 0.4886\n",
      "Epoch [4/11], Step [36/81], Loss: 0.4741\n",
      "Epoch [4/11], Step [37/81], Loss: 0.6002\n",
      "Epoch [4/11], Step [38/81], Loss: 0.6974\n",
      "Epoch [4/11], Step [39/81], Loss: 0.6121\n",
      "Epoch [4/11], Step [40/81], Loss: 0.6754\n",
      "Epoch [4/11], Step [41/81], Loss: 0.4457\n",
      "Epoch [4/11], Step [42/81], Loss: 0.7283\n",
      "Epoch [4/11], Step [43/81], Loss: 0.3601\n",
      "Epoch [4/11], Step [44/81], Loss: 0.4551\n",
      "Epoch [4/11], Step [45/81], Loss: 0.5826\n",
      "Epoch [4/11], Step [46/81], Loss: 0.5601\n",
      "Epoch [4/11], Step [47/81], Loss: 0.4785\n",
      "Epoch [4/11], Step [48/81], Loss: 0.4491\n",
      "Epoch [4/11], Step [49/81], Loss: 0.4055\n",
      "Epoch [4/11], Step [50/81], Loss: 0.6407\n",
      "Epoch [4/11], Step [51/81], Loss: 0.5406\n",
      "Epoch [4/11], Step [52/81], Loss: 0.5768\n",
      "Epoch [4/11], Step [53/81], Loss: 0.7158\n",
      "Epoch [4/11], Step [54/81], Loss: 0.7940\n",
      "Epoch [4/11], Step [55/81], Loss: 0.4852\n",
      "Epoch [4/11], Step [56/81], Loss: 0.4214\n",
      "Epoch [4/11], Step [57/81], Loss: 0.8220\n",
      "Epoch [4/11], Step [58/81], Loss: 0.7180\n",
      "Epoch [4/11], Step [59/81], Loss: 0.5626\n",
      "Epoch [4/11], Step [60/81], Loss: 0.9209\n",
      "Epoch [4/11], Step [61/81], Loss: 0.6307\n",
      "Epoch [4/11], Step [62/81], Loss: 0.4598\n",
      "Epoch [4/11], Step [63/81], Loss: 0.7974\n",
      "Epoch [4/11], Step [64/81], Loss: 0.5995\n",
      "Epoch [4/11], Step [65/81], Loss: 0.6940\n",
      "Epoch [4/11], Step [66/81], Loss: 0.2739\n",
      "Epoch [4/11], Step [67/81], Loss: 0.4805\n",
      "Epoch [4/11], Step [68/81], Loss: 0.4568\n",
      "Epoch [4/11], Step [69/81], Loss: 0.9885\n",
      "Epoch [4/11], Step [70/81], Loss: 0.4633\n",
      "Epoch [4/11], Step [71/81], Loss: 0.7410\n",
      "Epoch [4/11], Step [72/81], Loss: 0.5325\n",
      "Epoch [4/11], Step [73/81], Loss: 0.4791\n",
      "Epoch [4/11], Step [74/81], Loss: 0.8190\n",
      "Epoch [4/11], Step [75/81], Loss: 0.7975\n",
      "Epoch [4/11], Step [76/81], Loss: 0.6548\n",
      "Epoch [4/11], Step [77/81], Loss: 0.5824\n",
      "Epoch [4/11], Step [78/81], Loss: 0.6266\n",
      "Epoch [4/11], Step [79/81], Loss: 0.3926\n",
      "Epoch [4/11], Step [80/81], Loss: 0.5710\n",
      "Epoch [4/11], Step [81/81], Loss: 0.8775\n",
      "Epoch [5/11], Step [1/81], Loss: 0.6104\n",
      "Epoch [5/11], Step [2/81], Loss: 0.5126\n",
      "Epoch [5/11], Step [3/81], Loss: 0.5852\n",
      "Epoch [5/11], Step [4/81], Loss: 0.8747\n",
      "Epoch [5/11], Step [5/81], Loss: 0.5248\n",
      "Epoch [5/11], Step [6/81], Loss: 0.5321\n",
      "Epoch [5/11], Step [7/81], Loss: 0.4238\n",
      "Epoch [5/11], Step [8/81], Loss: 0.7241\n",
      "Epoch [5/11], Step [9/81], Loss: 0.5288\n",
      "Epoch [5/11], Step [10/81], Loss: 0.4419\n",
      "Epoch [5/11], Step [11/81], Loss: 0.6084\n",
      "Epoch [5/11], Step [12/81], Loss: 0.6166\n",
      "Epoch [5/11], Step [13/81], Loss: 0.4261\n",
      "Epoch [5/11], Step [14/81], Loss: 0.7218\n",
      "Epoch [5/11], Step [15/81], Loss: 0.5225\n",
      "Epoch [5/11], Step [16/81], Loss: 0.5051\n",
      "Epoch [5/11], Step [17/81], Loss: 0.6907\n",
      "Epoch [5/11], Step [18/81], Loss: 0.5871\n",
      "Epoch [5/11], Step [19/81], Loss: 0.5904\n",
      "Epoch [5/11], Step [20/81], Loss: 0.4976\n",
      "Epoch [5/11], Step [21/81], Loss: 0.5426\n",
      "Epoch [5/11], Step [22/81], Loss: 0.5224\n",
      "Epoch [5/11], Step [23/81], Loss: 0.4329\n",
      "Epoch [5/11], Step [24/81], Loss: 0.7367\n",
      "Epoch [5/11], Step [25/81], Loss: 0.6497\n",
      "Epoch [5/11], Step [26/81], Loss: 0.5849\n",
      "Epoch [5/11], Step [27/81], Loss: 0.5178\n",
      "Epoch [5/11], Step [28/81], Loss: 0.5167\n",
      "Epoch [5/11], Step [29/81], Loss: 0.7738\n",
      "Epoch [5/11], Step [30/81], Loss: 0.5988\n",
      "Epoch [5/11], Step [31/81], Loss: 0.5757\n",
      "Epoch [5/11], Step [32/81], Loss: 0.7663\n",
      "Epoch [5/11], Step [33/81], Loss: 0.6124\n",
      "Epoch [5/11], Step [34/81], Loss: 0.6408\n",
      "Epoch [5/11], Step [35/81], Loss: 0.7755\n",
      "Epoch [5/11], Step [36/81], Loss: 0.4144\n",
      "Epoch [5/11], Step [37/81], Loss: 0.3379\n",
      "Epoch [5/11], Step [38/81], Loss: 0.5959\n",
      "Epoch [5/11], Step [39/81], Loss: 0.3045\n",
      "Epoch [5/11], Step [40/81], Loss: 0.5111\n",
      "Epoch [5/11], Step [41/81], Loss: 0.3705\n",
      "Epoch [5/11], Step [42/81], Loss: 0.4395\n",
      "Epoch [5/11], Step [43/81], Loss: 0.3041\n",
      "Epoch [5/11], Step [44/81], Loss: 0.4653\n",
      "Epoch [5/11], Step [45/81], Loss: 0.5507\n",
      "Epoch [5/11], Step [46/81], Loss: 0.5329\n",
      "Epoch [5/11], Step [47/81], Loss: 0.8239\n",
      "Epoch [5/11], Step [48/81], Loss: 0.5650\n",
      "Epoch [5/11], Step [49/81], Loss: 0.4338\n",
      "Epoch [5/11], Step [50/81], Loss: 0.5723\n",
      "Epoch [5/11], Step [51/81], Loss: 0.6909\n",
      "Epoch [5/11], Step [52/81], Loss: 0.8526\n",
      "Epoch [5/11], Step [53/81], Loss: 0.5325\n",
      "Epoch [5/11], Step [54/81], Loss: 0.7287\n",
      "Epoch [5/11], Step [55/81], Loss: 1.0048\n",
      "Epoch [5/11], Step [56/81], Loss: 0.6765\n",
      "Epoch [5/11], Step [57/81], Loss: 0.3645\n",
      "Epoch [5/11], Step [58/81], Loss: 0.7166\n",
      "Epoch [5/11], Step [59/81], Loss: 1.0353\n",
      "Epoch [5/11], Step [60/81], Loss: 0.7218\n",
      "Epoch [5/11], Step [61/81], Loss: 0.6615\n",
      "Epoch [5/11], Step [62/81], Loss: 0.7072\n",
      "Epoch [5/11], Step [63/81], Loss: 0.8514\n",
      "Epoch [5/11], Step [64/81], Loss: 0.4653\n",
      "Epoch [5/11], Step [65/81], Loss: 0.7098\n",
      "Epoch [5/11], Step [66/81], Loss: 0.5427\n",
      "Epoch [5/11], Step [67/81], Loss: 0.4799\n",
      "Epoch [5/11], Step [68/81], Loss: 0.3941\n",
      "Epoch [5/11], Step [69/81], Loss: 0.5148\n",
      "Epoch [5/11], Step [70/81], Loss: 0.7917\n",
      "Epoch [5/11], Step [71/81], Loss: 0.8357\n",
      "Epoch [5/11], Step [72/81], Loss: 0.6359\n",
      "Epoch [5/11], Step [73/81], Loss: 0.5946\n",
      "Epoch [5/11], Step [74/81], Loss: 0.6450\n",
      "Epoch [5/11], Step [75/81], Loss: 0.3935\n",
      "Epoch [5/11], Step [76/81], Loss: 0.3709\n",
      "Epoch [5/11], Step [77/81], Loss: 0.3220\n",
      "Epoch [5/11], Step [78/81], Loss: 0.6307\n",
      "Epoch [5/11], Step [79/81], Loss: 0.4365\n",
      "Epoch [5/11], Step [80/81], Loss: 0.4152\n",
      "Epoch [5/11], Step [81/81], Loss: 0.7448\n",
      "Epoch [6/11], Step [1/81], Loss: 0.4096\n",
      "Epoch [6/11], Step [2/81], Loss: 0.4786\n",
      "Epoch [6/11], Step [3/81], Loss: 0.3599\n",
      "Epoch [6/11], Step [4/81], Loss: 0.4209\n",
      "Epoch [6/11], Step [5/81], Loss: 0.3214\n",
      "Epoch [6/11], Step [6/81], Loss: 0.2736\n",
      "Epoch [6/11], Step [7/81], Loss: 0.4902\n",
      "Epoch [6/11], Step [8/81], Loss: 0.5115\n",
      "Epoch [6/11], Step [9/81], Loss: 0.2761\n",
      "Epoch [6/11], Step [10/81], Loss: 0.5075\n",
      "Epoch [6/11], Step [11/81], Loss: 0.4397\n",
      "Epoch [6/11], Step [12/81], Loss: 0.4273\n",
      "Epoch [6/11], Step [13/81], Loss: 0.3522\n",
      "Epoch [6/11], Step [14/81], Loss: 0.2857\n",
      "Epoch [6/11], Step [15/81], Loss: 0.5631\n",
      "Epoch [6/11], Step [16/81], Loss: 0.3844\n",
      "Epoch [6/11], Step [17/81], Loss: 0.2994\n",
      "Epoch [6/11], Step [18/81], Loss: 0.5146\n",
      "Epoch [6/11], Step [19/81], Loss: 0.4483\n",
      "Epoch [6/11], Step [20/81], Loss: 0.3941\n",
      "Epoch [6/11], Step [21/81], Loss: 0.4379\n",
      "Epoch [6/11], Step [22/81], Loss: 0.4313\n",
      "Epoch [6/11], Step [23/81], Loss: 0.6524\n",
      "Epoch [6/11], Step [24/81], Loss: 0.4895\n",
      "Epoch [6/11], Step [25/81], Loss: 0.3483\n",
      "Epoch [6/11], Step [26/81], Loss: 0.5099\n",
      "Epoch [6/11], Step [27/81], Loss: 0.6347\n",
      "Epoch [6/11], Step [28/81], Loss: 0.4997\n",
      "Epoch [6/11], Step [29/81], Loss: 0.4581\n",
      "Epoch [6/11], Step [30/81], Loss: 0.7851\n",
      "Epoch [6/11], Step [31/81], Loss: 0.5397\n",
      "Epoch [6/11], Step [32/81], Loss: 0.5563\n",
      "Epoch [6/11], Step [33/81], Loss: 0.7157\n",
      "Epoch [6/11], Step [34/81], Loss: 0.4163\n",
      "Epoch [6/11], Step [35/81], Loss: 0.3214\n",
      "Epoch [6/11], Step [36/81], Loss: 0.6098\n",
      "Epoch [6/11], Step [37/81], Loss: 0.5908\n",
      "Epoch [6/11], Step [38/81], Loss: 0.5237\n",
      "Epoch [6/11], Step [39/81], Loss: 0.5718\n",
      "Epoch [6/11], Step [40/81], Loss: 0.7359\n",
      "Epoch [6/11], Step [41/81], Loss: 0.3997\n",
      "Epoch [6/11], Step [42/81], Loss: 0.5816\n",
      "Epoch [6/11], Step [43/81], Loss: 0.3933\n",
      "Epoch [6/11], Step [44/81], Loss: 0.6235\n",
      "Epoch [6/11], Step [45/81], Loss: 0.4379\n",
      "Epoch [6/11], Step [46/81], Loss: 0.3538\n",
      "Epoch [6/11], Step [47/81], Loss: 0.5060\n",
      "Epoch [6/11], Step [48/81], Loss: 0.5867\n",
      "Epoch [6/11], Step [49/81], Loss: 0.4813\n",
      "Epoch [6/11], Step [50/81], Loss: 0.4465\n",
      "Epoch [6/11], Step [51/81], Loss: 0.6176\n",
      "Epoch [6/11], Step [52/81], Loss: 0.3051\n",
      "Epoch [6/11], Step [53/81], Loss: 0.4217\n",
      "Epoch [6/11], Step [54/81], Loss: 0.3399\n",
      "Epoch [6/11], Step [55/81], Loss: 0.3828\n",
      "Epoch [6/11], Step [56/81], Loss: 0.5586\n",
      "Epoch [6/11], Step [57/81], Loss: 0.3883\n",
      "Epoch [6/11], Step [58/81], Loss: 0.5202\n",
      "Epoch [6/11], Step [59/81], Loss: 0.6352\n",
      "Epoch [6/11], Step [60/81], Loss: 0.6989\n",
      "Epoch [6/11], Step [61/81], Loss: 0.6083\n",
      "Epoch [6/11], Step [62/81], Loss: 0.7185\n",
      "Epoch [6/11], Step [63/81], Loss: 0.3809\n",
      "Epoch [6/11], Step [64/81], Loss: 0.4932\n",
      "Epoch [6/11], Step [65/81], Loss: 0.2524\n",
      "Epoch [6/11], Step [66/81], Loss: 0.3132\n",
      "Epoch [6/11], Step [67/81], Loss: 0.4243\n",
      "Epoch [6/11], Step [68/81], Loss: 0.6390\n",
      "Epoch [6/11], Step [69/81], Loss: 0.5311\n",
      "Epoch [6/11], Step [70/81], Loss: 0.4189\n",
      "Epoch [6/11], Step [71/81], Loss: 0.7063\n",
      "Epoch [6/11], Step [72/81], Loss: 0.4453\n",
      "Epoch [6/11], Step [73/81], Loss: 0.4800\n",
      "Epoch [6/11], Step [74/81], Loss: 0.4659\n",
      "Epoch [6/11], Step [75/81], Loss: 0.2440\n",
      "Epoch [6/11], Step [76/81], Loss: 0.7459\n",
      "Epoch [6/11], Step [77/81], Loss: 0.2791\n",
      "Epoch [6/11], Step [78/81], Loss: 0.5995\n",
      "Epoch [6/11], Step [79/81], Loss: 0.3521\n",
      "Epoch [6/11], Step [80/81], Loss: 0.4485\n",
      "Epoch [6/11], Step [81/81], Loss: 0.1582\n",
      "Epoch [7/11], Step [1/81], Loss: 0.7310\n",
      "Epoch [7/11], Step [2/81], Loss: 0.3334\n",
      "Epoch [7/11], Step [3/81], Loss: 0.4859\n",
      "Epoch [7/11], Step [4/81], Loss: 0.2571\n",
      "Epoch [7/11], Step [5/81], Loss: 0.8410\n",
      "Epoch [7/11], Step [6/81], Loss: 0.7874\n",
      "Epoch [7/11], Step [7/81], Loss: 0.2457\n",
      "Epoch [7/11], Step [8/81], Loss: 0.6622\n",
      "Epoch [7/11], Step [9/81], Loss: 0.7736\n",
      "Epoch [7/11], Step [10/81], Loss: 0.5033\n",
      "Epoch [7/11], Step [11/81], Loss: 0.3501\n",
      "Epoch [7/11], Step [12/81], Loss: 0.4166\n",
      "Epoch [7/11], Step [13/81], Loss: 0.3924\n",
      "Epoch [7/11], Step [14/81], Loss: 0.6307\n",
      "Epoch [7/11], Step [15/81], Loss: 0.6399\n",
      "Epoch [7/11], Step [16/81], Loss: 0.4457\n",
      "Epoch [7/11], Step [17/81], Loss: 0.5394\n",
      "Epoch [7/11], Step [18/81], Loss: 0.4843\n",
      "Epoch [7/11], Step [19/81], Loss: 0.3372\n",
      "Epoch [7/11], Step [20/81], Loss: 0.4301\n",
      "Epoch [7/11], Step [21/81], Loss: 0.3996\n",
      "Epoch [7/11], Step [22/81], Loss: 0.4467\n",
      "Epoch [7/11], Step [23/81], Loss: 0.6154\n",
      "Epoch [7/11], Step [24/81], Loss: 0.5696\n",
      "Epoch [7/11], Step [25/81], Loss: 0.6652\n",
      "Epoch [7/11], Step [26/81], Loss: 0.4217\n",
      "Epoch [7/11], Step [27/81], Loss: 0.2239\n",
      "Epoch [7/11], Step [28/81], Loss: 0.3671\n",
      "Epoch [7/11], Step [29/81], Loss: 0.8559\n",
      "Epoch [7/11], Step [30/81], Loss: 0.2774\n",
      "Epoch [7/11], Step [31/81], Loss: 0.2328\n",
      "Epoch [7/11], Step [32/81], Loss: 0.5691\n",
      "Epoch [7/11], Step [33/81], Loss: 0.3298\n",
      "Epoch [7/11], Step [34/81], Loss: 0.4687\n",
      "Epoch [7/11], Step [35/81], Loss: 0.3398\n",
      "Epoch [7/11], Step [36/81], Loss: 0.3903\n",
      "Epoch [7/11], Step [37/81], Loss: 0.2849\n",
      "Epoch [7/11], Step [38/81], Loss: 0.1214\n",
      "Epoch [7/11], Step [39/81], Loss: 0.3308\n",
      "Epoch [7/11], Step [40/81], Loss: 0.5026\n",
      "Epoch [7/11], Step [41/81], Loss: 0.1857\n",
      "Epoch [7/11], Step [42/81], Loss: 0.4500\n",
      "Epoch [7/11], Step [43/81], Loss: 0.6080\n",
      "Epoch [7/11], Step [44/81], Loss: 0.5152\n",
      "Epoch [7/11], Step [45/81], Loss: 0.5596\n",
      "Epoch [7/11], Step [46/81], Loss: 0.3630\n",
      "Epoch [7/11], Step [47/81], Loss: 0.3323\n",
      "Epoch [7/11], Step [48/81], Loss: 0.3818\n",
      "Epoch [7/11], Step [49/81], Loss: 0.2256\n",
      "Epoch [7/11], Step [50/81], Loss: 0.3198\n",
      "Epoch [7/11], Step [51/81], Loss: 0.7002\n",
      "Epoch [7/11], Step [52/81], Loss: 0.1639\n",
      "Epoch [7/11], Step [53/81], Loss: 0.3621\n",
      "Epoch [7/11], Step [54/81], Loss: 0.4043\n",
      "Epoch [7/11], Step [55/81], Loss: 0.2986\n",
      "Epoch [7/11], Step [56/81], Loss: 0.3742\n",
      "Epoch [7/11], Step [57/81], Loss: 0.4124\n",
      "Epoch [7/11], Step [58/81], Loss: 0.2228\n",
      "Epoch [7/11], Step [59/81], Loss: 0.4559\n",
      "Epoch [7/11], Step [60/81], Loss: 0.3426\n",
      "Epoch [7/11], Step [61/81], Loss: 0.2296\n",
      "Epoch [7/11], Step [62/81], Loss: 0.6531\n",
      "Epoch [7/11], Step [63/81], Loss: 0.6340\n",
      "Epoch [7/11], Step [64/81], Loss: 0.2568\n",
      "Epoch [7/11], Step [65/81], Loss: 0.4243\n",
      "Epoch [7/11], Step [66/81], Loss: 0.6541\n",
      "Epoch [7/11], Step [67/81], Loss: 0.3988\n",
      "Epoch [7/11], Step [68/81], Loss: 0.3850\n",
      "Epoch [7/11], Step [69/81], Loss: 0.2906\n",
      "Epoch [7/11], Step [70/81], Loss: 0.4338\n",
      "Epoch [7/11], Step [71/81], Loss: 0.8690\n",
      "Epoch [7/11], Step [72/81], Loss: 0.4099\n",
      "Epoch [7/11], Step [73/81], Loss: 0.4146\n",
      "Epoch [7/11], Step [74/81], Loss: 0.3834\n",
      "Epoch [7/11], Step [75/81], Loss: 0.4424\n",
      "Epoch [7/11], Step [76/81], Loss: 0.4678\n",
      "Epoch [7/11], Step [77/81], Loss: 0.2979\n",
      "Epoch [7/11], Step [78/81], Loss: 0.3857\n",
      "Epoch [7/11], Step [79/81], Loss: 0.4263\n",
      "Epoch [7/11], Step [80/81], Loss: 0.3168\n",
      "Epoch [7/11], Step [81/81], Loss: 0.2272\n",
      "Epoch [8/11], Step [1/81], Loss: 0.1699\n",
      "Epoch [8/11], Step [2/81], Loss: 0.4212\n",
      "Epoch [8/11], Step [3/81], Loss: 0.4075\n",
      "Epoch [8/11], Step [4/81], Loss: 0.4924\n",
      "Epoch [8/11], Step [5/81], Loss: 0.5492\n",
      "Epoch [8/11], Step [6/81], Loss: 0.3793\n",
      "Epoch [8/11], Step [7/81], Loss: 0.5027\n",
      "Epoch [8/11], Step [8/81], Loss: 0.4934\n",
      "Epoch [8/11], Step [9/81], Loss: 0.3624\n",
      "Epoch [8/11], Step [10/81], Loss: 0.2488\n",
      "Epoch [8/11], Step [11/81], Loss: 0.4246\n",
      "Epoch [8/11], Step [12/81], Loss: 0.2703\n",
      "Epoch [8/11], Step [13/81], Loss: 0.5580\n",
      "Epoch [8/11], Step [14/81], Loss: 0.3136\n",
      "Epoch [8/11], Step [15/81], Loss: 0.1473\n",
      "Epoch [8/11], Step [16/81], Loss: 0.1878\n",
      "Epoch [8/11], Step [17/81], Loss: 0.2819\n",
      "Epoch [8/11], Step [18/81], Loss: 0.4509\n",
      "Epoch [8/11], Step [19/81], Loss: 0.2390\n",
      "Epoch [8/11], Step [20/81], Loss: 0.3052\n",
      "Epoch [8/11], Step [21/81], Loss: 0.2186\n",
      "Epoch [8/11], Step [22/81], Loss: 0.3329\n",
      "Epoch [8/11], Step [23/81], Loss: 0.4164\n",
      "Epoch [8/11], Step [24/81], Loss: 0.3440\n",
      "Epoch [8/11], Step [25/81], Loss: 0.4024\n",
      "Epoch [8/11], Step [26/81], Loss: 0.5111\n",
      "Epoch [8/11], Step [27/81], Loss: 0.3624\n",
      "Epoch [8/11], Step [28/81], Loss: 0.4866\n",
      "Epoch [8/11], Step [29/81], Loss: 0.3962\n",
      "Epoch [8/11], Step [30/81], Loss: 0.4625\n",
      "Epoch [8/11], Step [31/81], Loss: 0.3623\n",
      "Epoch [8/11], Step [32/81], Loss: 0.3994\n",
      "Epoch [8/11], Step [33/81], Loss: 0.4710\n",
      "Epoch [8/11], Step [34/81], Loss: 0.4224\n",
      "Epoch [8/11], Step [35/81], Loss: 0.2624\n",
      "Epoch [8/11], Step [36/81], Loss: 0.4623\n",
      "Epoch [8/11], Step [37/81], Loss: 0.1753\n",
      "Epoch [8/11], Step [38/81], Loss: 0.1889\n",
      "Epoch [8/11], Step [39/81], Loss: 0.4594\n",
      "Epoch [8/11], Step [40/81], Loss: 0.6566\n",
      "Epoch [8/11], Step [41/81], Loss: 0.3538\n",
      "Epoch [8/11], Step [42/81], Loss: 0.3195\n",
      "Epoch [8/11], Step [43/81], Loss: 0.5349\n",
      "Epoch [8/11], Step [44/81], Loss: 0.4120\n",
      "Epoch [8/11], Step [45/81], Loss: 0.8490\n",
      "Epoch [8/11], Step [46/81], Loss: 0.2618\n",
      "Epoch [8/11], Step [47/81], Loss: 0.2774\n",
      "Epoch [8/11], Step [48/81], Loss: 0.6396\n",
      "Epoch [8/11], Step [49/81], Loss: 0.4365\n",
      "Epoch [8/11], Step [50/81], Loss: 0.4212\n",
      "Epoch [8/11], Step [51/81], Loss: 0.7300\n",
      "Epoch [8/11], Step [52/81], Loss: 0.5614\n",
      "Epoch [8/11], Step [53/81], Loss: 0.4066\n",
      "Epoch [8/11], Step [54/81], Loss: 0.2691\n",
      "Epoch [8/11], Step [55/81], Loss: 0.4263\n",
      "Epoch [8/11], Step [56/81], Loss: 0.2463\n",
      "Epoch [8/11], Step [57/81], Loss: 0.4554\n",
      "Epoch [8/11], Step [58/81], Loss: 0.3778\n",
      "Epoch [8/11], Step [59/81], Loss: 0.6058\n",
      "Epoch [8/11], Step [60/81], Loss: 0.3479\n",
      "Epoch [8/11], Step [61/81], Loss: 0.3033\n",
      "Epoch [8/11], Step [62/81], Loss: 0.2729\n",
      "Epoch [8/11], Step [63/81], Loss: 0.3181\n",
      "Epoch [8/11], Step [64/81], Loss: 0.3882\n",
      "Epoch [8/11], Step [65/81], Loss: 0.4896\n",
      "Epoch [8/11], Step [66/81], Loss: 0.3528\n",
      "Epoch [8/11], Step [67/81], Loss: 0.3185\n",
      "Epoch [8/11], Step [68/81], Loss: 0.5229\n",
      "Epoch [8/11], Step [69/81], Loss: 0.5226\n",
      "Epoch [8/11], Step [70/81], Loss: 0.1643\n",
      "Epoch [8/11], Step [71/81], Loss: 0.4098\n",
      "Epoch [8/11], Step [72/81], Loss: 0.2142\n",
      "Epoch [8/11], Step [73/81], Loss: 0.2416\n",
      "Epoch [8/11], Step [74/81], Loss: 0.2483\n",
      "Epoch [8/11], Step [75/81], Loss: 0.3720\n",
      "Epoch [8/11], Step [76/81], Loss: 0.3345\n",
      "Epoch [8/11], Step [77/81], Loss: 0.2415\n",
      "Epoch [8/11], Step [78/81], Loss: 0.3226\n",
      "Epoch [8/11], Step [79/81], Loss: 0.5021\n",
      "Epoch [8/11], Step [80/81], Loss: 0.3295\n",
      "Epoch [8/11], Step [81/81], Loss: 0.4651\n",
      "Epoch [9/11], Step [1/81], Loss: 0.1839\n",
      "Epoch [9/11], Step [2/81], Loss: 0.2010\n",
      "Epoch [9/11], Step [3/81], Loss: 0.2301\n",
      "Epoch [9/11], Step [4/81], Loss: 0.2668\n",
      "Epoch [9/11], Step [5/81], Loss: 0.1996\n",
      "Epoch [9/11], Step [6/81], Loss: 0.4213\n",
      "Epoch [9/11], Step [7/81], Loss: 0.3089\n",
      "Epoch [9/11], Step [8/81], Loss: 0.2617\n",
      "Epoch [9/11], Step [9/81], Loss: 0.2345\n",
      "Epoch [9/11], Step [10/81], Loss: 0.2359\n",
      "Epoch [9/11], Step [11/81], Loss: 0.3742\n",
      "Epoch [9/11], Step [12/81], Loss: 0.3208\n",
      "Epoch [9/11], Step [13/81], Loss: 0.2076\n",
      "Epoch [9/11], Step [14/81], Loss: 0.3710\n",
      "Epoch [9/11], Step [15/81], Loss: 0.3631\n",
      "Epoch [9/11], Step [16/81], Loss: 0.4632\n",
      "Epoch [9/11], Step [17/81], Loss: 0.6301\n",
      "Epoch [9/11], Step [18/81], Loss: 0.3235\n",
      "Epoch [9/11], Step [19/81], Loss: 0.1404\n",
      "Epoch [9/11], Step [20/81], Loss: 0.2682\n",
      "Epoch [9/11], Step [21/81], Loss: 0.8407\n",
      "Epoch [9/11], Step [22/81], Loss: 0.4219\n",
      "Epoch [9/11], Step [23/81], Loss: 0.2083\n",
      "Epoch [9/11], Step [24/81], Loss: 0.3206\n",
      "Epoch [9/11], Step [25/81], Loss: 0.1306\n",
      "Epoch [9/11], Step [26/81], Loss: 0.0954\n",
      "Epoch [9/11], Step [27/81], Loss: 0.3624\n",
      "Epoch [9/11], Step [28/81], Loss: 0.5820\n",
      "Epoch [9/11], Step [29/81], Loss: 0.3475\n",
      "Epoch [9/11], Step [30/81], Loss: 0.4169\n",
      "Epoch [9/11], Step [31/81], Loss: 0.6259\n",
      "Epoch [9/11], Step [32/81], Loss: 0.5021\n",
      "Epoch [9/11], Step [33/81], Loss: 0.1338\n",
      "Epoch [9/11], Step [34/81], Loss: 0.3532\n",
      "Epoch [9/11], Step [35/81], Loss: 0.2090\n",
      "Epoch [9/11], Step [36/81], Loss: 0.2678\n",
      "Epoch [9/11], Step [37/81], Loss: 0.3427\n",
      "Epoch [9/11], Step [38/81], Loss: 0.2815\n",
      "Epoch [9/11], Step [39/81], Loss: 0.2960\n",
      "Epoch [9/11], Step [40/81], Loss: 0.6700\n",
      "Epoch [9/11], Step [41/81], Loss: 0.2291\n",
      "Epoch [9/11], Step [42/81], Loss: 0.2830\n",
      "Epoch [9/11], Step [43/81], Loss: 0.5697\n",
      "Epoch [9/11], Step [44/81], Loss: 0.2589\n",
      "Epoch [9/11], Step [45/81], Loss: 0.4130\n",
      "Epoch [9/11], Step [46/81], Loss: 0.3360\n",
      "Epoch [9/11], Step [47/81], Loss: 0.3469\n",
      "Epoch [9/11], Step [48/81], Loss: 0.2928\n",
      "Epoch [9/11], Step [49/81], Loss: 0.4615\n",
      "Epoch [9/11], Step [50/81], Loss: 0.3848\n",
      "Epoch [9/11], Step [51/81], Loss: 0.4440\n",
      "Epoch [9/11], Step [52/81], Loss: 0.1772\n",
      "Epoch [9/11], Step [53/81], Loss: 0.4538\n",
      "Epoch [9/11], Step [54/81], Loss: 0.2949\n",
      "Epoch [9/11], Step [55/81], Loss: 0.3676\n",
      "Epoch [9/11], Step [56/81], Loss: 0.3289\n",
      "Epoch [9/11], Step [57/81], Loss: 0.5135\n",
      "Epoch [9/11], Step [58/81], Loss: 0.3143\n",
      "Epoch [9/11], Step [59/81], Loss: 0.3357\n",
      "Epoch [9/11], Step [60/81], Loss: 0.2256\n",
      "Epoch [9/11], Step [61/81], Loss: 0.6333\n",
      "Epoch [9/11], Step [62/81], Loss: 0.2154\n",
      "Epoch [9/11], Step [63/81], Loss: 0.1258\n",
      "Epoch [9/11], Step [64/81], Loss: 0.5032\n",
      "Epoch [9/11], Step [65/81], Loss: 0.4645\n",
      "Epoch [9/11], Step [66/81], Loss: 0.3322\n",
      "Epoch [9/11], Step [67/81], Loss: 0.3322\n",
      "Epoch [9/11], Step [68/81], Loss: 0.3783\n",
      "Epoch [9/11], Step [69/81], Loss: 0.2036\n",
      "Epoch [9/11], Step [70/81], Loss: 0.2428\n",
      "Epoch [9/11], Step [71/81], Loss: 0.2168\n",
      "Epoch [9/11], Step [72/81], Loss: 0.2434\n",
      "Epoch [9/11], Step [73/81], Loss: 0.2332\n",
      "Epoch [9/11], Step [74/81], Loss: 0.3396\n",
      "Epoch [9/11], Step [75/81], Loss: 0.2940\n",
      "Epoch [9/11], Step [76/81], Loss: 0.3179\n",
      "Epoch [9/11], Step [77/81], Loss: 0.3119\n",
      "Epoch [9/11], Step [78/81], Loss: 0.4041\n",
      "Epoch [9/11], Step [79/81], Loss: 0.2558\n",
      "Epoch [9/11], Step [80/81], Loss: 0.4295\n",
      "Epoch [9/11], Step [81/81], Loss: 0.1087\n",
      "Epoch [10/11], Step [1/81], Loss: 0.2756\n",
      "Epoch [10/11], Step [2/81], Loss: 0.2924\n",
      "Epoch [10/11], Step [3/81], Loss: 0.2029\n",
      "Epoch [10/11], Step [4/81], Loss: 0.1843\n",
      "Epoch [10/11], Step [5/81], Loss: 0.1453\n",
      "Epoch [10/11], Step [6/81], Loss: 0.3116\n",
      "Epoch [10/11], Step [7/81], Loss: 0.3247\n",
      "Epoch [10/11], Step [8/81], Loss: 0.3967\n",
      "Epoch [10/11], Step [9/81], Loss: 0.4264\n",
      "Epoch [10/11], Step [10/81], Loss: 0.1821\n",
      "Epoch [10/11], Step [11/81], Loss: 0.2810\n",
      "Epoch [10/11], Step [12/81], Loss: 0.2168\n",
      "Epoch [10/11], Step [13/81], Loss: 0.5547\n",
      "Epoch [10/11], Step [14/81], Loss: 0.4347\n",
      "Epoch [10/11], Step [15/81], Loss: 0.4547\n",
      "Epoch [10/11], Step [16/81], Loss: 0.2557\n",
      "Epoch [10/11], Step [17/81], Loss: 0.4499\n",
      "Epoch [10/11], Step [18/81], Loss: 0.1793\n",
      "Epoch [10/11], Step [19/81], Loss: 0.3491\n",
      "Epoch [10/11], Step [20/81], Loss: 0.3900\n",
      "Epoch [10/11], Step [21/81], Loss: 0.4690\n",
      "Epoch [10/11], Step [22/81], Loss: 0.4156\n",
      "Epoch [10/11], Step [23/81], Loss: 0.2780\n",
      "Epoch [10/11], Step [24/81], Loss: 0.2044\n",
      "Epoch [10/11], Step [25/81], Loss: 0.1580\n",
      "Epoch [10/11], Step [26/81], Loss: 0.3088\n",
      "Epoch [10/11], Step [27/81], Loss: 0.2369\n",
      "Epoch [10/11], Step [28/81], Loss: 0.3224\n",
      "Epoch [10/11], Step [29/81], Loss: 0.2898\n",
      "Epoch [10/11], Step [30/81], Loss: 0.4556\n",
      "Epoch [10/11], Step [31/81], Loss: 0.5005\n",
      "Epoch [10/11], Step [32/81], Loss: 0.1441\n",
      "Epoch [10/11], Step [33/81], Loss: 0.2171\n",
      "Epoch [10/11], Step [34/81], Loss: 0.3424\n",
      "Epoch [10/11], Step [35/81], Loss: 0.4562\n",
      "Epoch [10/11], Step [36/81], Loss: 0.3665\n",
      "Epoch [10/11], Step [37/81], Loss: 0.3209\n",
      "Epoch [10/11], Step [38/81], Loss: 0.2502\n",
      "Epoch [10/11], Step [39/81], Loss: 0.6890\n",
      "Epoch [10/11], Step [40/81], Loss: 0.2960\n",
      "Epoch [10/11], Step [41/81], Loss: 0.5494\n",
      "Epoch [10/11], Step [42/81], Loss: 0.2365\n",
      "Epoch [10/11], Step [43/81], Loss: 0.4540\n",
      "Epoch [10/11], Step [44/81], Loss: 0.3681\n",
      "Epoch [10/11], Step [45/81], Loss: 0.2359\n",
      "Epoch [10/11], Step [46/81], Loss: 0.2162\n",
      "Epoch [10/11], Step [47/81], Loss: 0.2827\n",
      "Epoch [10/11], Step [48/81], Loss: 0.3223\n",
      "Epoch [10/11], Step [49/81], Loss: 0.3931\n",
      "Epoch [10/11], Step [50/81], Loss: 0.1789\n",
      "Epoch [10/11], Step [51/81], Loss: 0.2349\n",
      "Epoch [10/11], Step [52/81], Loss: 0.3896\n",
      "Epoch [10/11], Step [53/81], Loss: 0.3060\n",
      "Epoch [10/11], Step [54/81], Loss: 0.2960\n",
      "Epoch [10/11], Step [55/81], Loss: 0.1568\n",
      "Epoch [10/11], Step [56/81], Loss: 0.5405\n",
      "Epoch [10/11], Step [57/81], Loss: 0.2199\n",
      "Epoch [10/11], Step [58/81], Loss: 0.3260\n",
      "Epoch [10/11], Step [59/81], Loss: 0.2356\n",
      "Epoch [10/11], Step [60/81], Loss: 0.1278\n",
      "Epoch [10/11], Step [61/81], Loss: 0.3513\n",
      "Epoch [10/11], Step [62/81], Loss: 0.4153\n",
      "Epoch [10/11], Step [63/81], Loss: 0.3611\n",
      "Epoch [10/11], Step [64/81], Loss: 0.3919\n",
      "Epoch [10/11], Step [65/81], Loss: 0.4749\n",
      "Epoch [10/11], Step [66/81], Loss: 0.2256\n",
      "Epoch [10/11], Step [67/81], Loss: 0.3415\n",
      "Epoch [10/11], Step [68/81], Loss: 0.4100\n",
      "Epoch [10/11], Step [69/81], Loss: 0.4857\n",
      "Epoch [10/11], Step [70/81], Loss: 0.1019\n",
      "Epoch [10/11], Step [71/81], Loss: 0.5678\n",
      "Epoch [10/11], Step [72/81], Loss: 0.3476\n",
      "Epoch [10/11], Step [73/81], Loss: 0.4285\n",
      "Epoch [10/11], Step [74/81], Loss: 0.4104\n",
      "Epoch [10/11], Step [75/81], Loss: 0.4179\n",
      "Epoch [10/11], Step [76/81], Loss: 0.1318\n",
      "Epoch [10/11], Step [77/81], Loss: 0.1745\n",
      "Epoch [10/11], Step [78/81], Loss: 0.4404\n",
      "Epoch [10/11], Step [79/81], Loss: 0.3524\n",
      "Epoch [10/11], Step [80/81], Loss: 0.1788\n",
      "Epoch [10/11], Step [81/81], Loss: 0.4778\n",
      "Epoch [11/11], Step [1/81], Loss: 0.1823\n",
      "Epoch [11/11], Step [2/81], Loss: 0.1547\n",
      "Epoch [11/11], Step [3/81], Loss: 0.2436\n",
      "Epoch [11/11], Step [4/81], Loss: 0.3899\n",
      "Epoch [11/11], Step [5/81], Loss: 0.2436\n",
      "Epoch [11/11], Step [6/81], Loss: 0.3439\n",
      "Epoch [11/11], Step [7/81], Loss: 0.2120\n",
      "Epoch [11/11], Step [8/81], Loss: 0.3010\n",
      "Epoch [11/11], Step [9/81], Loss: 0.7466\n",
      "Epoch [11/11], Step [10/81], Loss: 0.3060\n",
      "Epoch [11/11], Step [11/81], Loss: 0.3607\n",
      "Epoch [11/11], Step [12/81], Loss: 0.2269\n",
      "Epoch [11/11], Step [13/81], Loss: 0.1788\n",
      "Epoch [11/11], Step [14/81], Loss: 0.4565\n",
      "Epoch [11/11], Step [15/81], Loss: 0.2413\n",
      "Epoch [11/11], Step [16/81], Loss: 0.2378\n",
      "Epoch [11/11], Step [17/81], Loss: 0.2748\n",
      "Epoch [11/11], Step [18/81], Loss: 0.3147\n",
      "Epoch [11/11], Step [19/81], Loss: 0.1328\n",
      "Epoch [11/11], Step [20/81], Loss: 0.3425\n",
      "Epoch [11/11], Step [21/81], Loss: 0.3324\n",
      "Epoch [11/11], Step [22/81], Loss: 0.5653\n",
      "Epoch [11/11], Step [23/81], Loss: 0.2312\n",
      "Epoch [11/11], Step [24/81], Loss: 0.5736\n",
      "Epoch [11/11], Step [25/81], Loss: 0.4060\n",
      "Epoch [11/11], Step [26/81], Loss: 0.3019\n",
      "Epoch [11/11], Step [27/81], Loss: 0.3792\n",
      "Epoch [11/11], Step [28/81], Loss: 0.1975\n",
      "Epoch [11/11], Step [29/81], Loss: 0.2385\n",
      "Epoch [11/11], Step [30/81], Loss: 0.1877\n",
      "Epoch [11/11], Step [31/81], Loss: 0.6652\n",
      "Epoch [11/11], Step [32/81], Loss: 0.2653\n",
      "Epoch [11/11], Step [33/81], Loss: 0.3261\n",
      "Epoch [11/11], Step [34/81], Loss: 0.2018\n",
      "Epoch [11/11], Step [35/81], Loss: 0.1758\n",
      "Epoch [11/11], Step [36/81], Loss: 0.2053\n",
      "Epoch [11/11], Step [37/81], Loss: 0.4657\n",
      "Epoch [11/11], Step [38/81], Loss: 0.6780\n",
      "Epoch [11/11], Step [39/81], Loss: 0.2506\n",
      "Epoch [11/11], Step [40/81], Loss: 0.1640\n",
      "Epoch [11/11], Step [41/81], Loss: 0.2071\n",
      "Epoch [11/11], Step [42/81], Loss: 0.2560\n",
      "Epoch [11/11], Step [43/81], Loss: 0.3901\n",
      "Epoch [11/11], Step [44/81], Loss: 0.3265\n",
      "Epoch [11/11], Step [45/81], Loss: 0.3971\n",
      "Epoch [11/11], Step [46/81], Loss: 0.1916\n",
      "Epoch [11/11], Step [47/81], Loss: 0.2913\n",
      "Epoch [11/11], Step [48/81], Loss: 0.2817\n",
      "Epoch [11/11], Step [49/81], Loss: 0.2686\n",
      "Epoch [11/11], Step [50/81], Loss: 0.1101\n",
      "Epoch [11/11], Step [51/81], Loss: 0.4283\n",
      "Epoch [11/11], Step [52/81], Loss: 0.2910\n",
      "Epoch [11/11], Step [53/81], Loss: 0.3645\n",
      "Epoch [11/11], Step [54/81], Loss: 0.1897\n",
      "Epoch [11/11], Step [55/81], Loss: 0.1651\n",
      "Epoch [11/11], Step [56/81], Loss: 0.0653\n",
      "Epoch [11/11], Step [57/81], Loss: 0.3047\n",
      "Epoch [11/11], Step [58/81], Loss: 0.1513\n",
      "Epoch [11/11], Step [59/81], Loss: 0.2269\n",
      "Epoch [11/11], Step [60/81], Loss: 0.0940\n",
      "Epoch [11/11], Step [61/81], Loss: 0.3629\n",
      "Epoch [11/11], Step [62/81], Loss: 0.6836\n",
      "Epoch [11/11], Step [63/81], Loss: 0.2296\n",
      "Epoch [11/11], Step [64/81], Loss: 0.3912\n",
      "Epoch [11/11], Step [65/81], Loss: 0.1903\n",
      "Epoch [11/11], Step [66/81], Loss: 0.1512\n",
      "Epoch [11/11], Step [67/81], Loss: 0.0855\n",
      "Epoch [11/11], Step [68/81], Loss: 0.1269\n",
      "Epoch [11/11], Step [69/81], Loss: 0.5354\n",
      "Epoch [11/11], Step [70/81], Loss: 0.2540\n",
      "Epoch [11/11], Step [71/81], Loss: 0.2879\n",
      "Epoch [11/11], Step [72/81], Loss: 0.4000\n",
      "Epoch [11/11], Step [73/81], Loss: 0.4757\n",
      "Epoch [11/11], Step [74/81], Loss: 0.2694\n",
      "Epoch [11/11], Step [75/81], Loss: 0.1992\n",
      "Epoch [11/11], Step [76/81], Loss: 0.1088\n",
      "Epoch [11/11], Step [77/81], Loss: 0.2686\n",
      "Epoch [11/11], Step [78/81], Loss: 0.2884\n",
      "Epoch [11/11], Step [79/81], Loss: 0.1964\n",
      "Epoch [11/11], Step [80/81], Loss: 0.1357\n",
      "Epoch [11/11], Step [81/81], Loss: 0.4069\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the loss and preciscion recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        90\n",
      "           1       0.89      0.82      0.86        91\n",
      "           2       0.90      0.88      0.89        41\n",
      "           3       0.82      0.92      0.87        65\n",
      "\n",
      "    accuracy                           0.88       287\n",
      "   macro avg       0.88      0.88      0.88       287\n",
      "weighted avg       0.88      0.88      0.88       287\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[81  7  1  1]\n",
      " [ 5 75  3  8]\n",
      " [ 0  1 36  4]\n",
      " [ 4  1  0 60]]\n",
      "\n",
      "Overall Accuracy: 0.8780\n"
     ]
    }
   ],
   "source": [
    "# Collect all predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Use the entire data loader\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(\n",
    "    all_labels, \n",
    "    all_preds, \n",
    "    zero_division=0  # Handles cases with no predictions for a class\n",
    "))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Use the entire data loader\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(\n",
    "    all_labels, \n",
    "    all_preds, \n",
    "    zero_division=0  # Handles cases with no predictions for a class\n",
    "))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
